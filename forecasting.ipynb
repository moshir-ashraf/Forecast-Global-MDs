{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData ():\n",
    "    csvFiles = [os.path.join(\"datasets\", filename) for filename in os.listdir(\"datasets\") if filename.endswith(('.csv'))]\n",
    "    data = []\n",
    "    for file in csvFiles:\n",
    "        frame = pd.read_csv(file).drop(['index'], axis=1)\n",
    "        if file == 'datasets\\Mental health Depression disorder Data.csv' :\n",
    "            frame = frame.loc[:6467]\n",
    "            for column in frame.columns:\n",
    "                if column != 'Entity' and column != 'Code':\n",
    "                    frame[column] = frame[column].astype(float)\n",
    "            frame['Year'] = frame['Year'].astype(int)\n",
    "        if 'Code' in frame.columns:\n",
    "            frame.drop(columns='Code',axis=1,inplace=True)\n",
    "        if 'Country' in frame.columns:\n",
    "            frame.rename(columns={'Country': 'Entity'}, inplace=True)\n",
    "        data.append(frame)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data,feature,scaler):\n",
    "    data[feature] = scaler.fit_transform(data[feature].values.reshape(-1, 1))\n",
    "    data[feature] = scaler.transform(data[feature].values.reshape(-1, 1))\n",
    "    data[feature] = scaler.transform(data[feature].values.reshape(-1, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode(data, feature):\n",
    "    encoded = pd.get_dummies(data[feature], prefix=feature)\n",
    "    data = pd.concat([data, encoded], axis=1)\n",
    "    data.drop(columns=[feature], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outlier Using Inter Quantile Range (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectOutliersIqr(feature):   \n",
    "    Q1 = np.percentile(feature, 25)\n",
    "    Q3 = np.percentile(feature, 75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    threshold = 1.5\n",
    "    lowerBound = Q1 - threshold * IQR\n",
    "    upperBound = Q3 + threshold * IQR\n",
    "\n",
    "    outlierIndices = feature[(feature < lowerBound) | (feature > upperBound)].index\n",
    "    mean = feature[(feature >= lowerBound) & (feature <= upperBound)].median()\n",
    "    feature.loc[outlierIndices] = mean\n",
    "    outlierCount = len(outlierIndices)\n",
    "    \n",
    "    return {\n",
    "        'featureName': feature.name,\n",
    "        'outlierCount': outlierCount\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decclaring the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_1996\\3597135414.py:5: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  frame = pd.read_csv(file).drop(['index'], axis=1)\n"
     ]
    }
   ],
   "source": [
    "data = loadData()\n",
    "data = yearlyAverage(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalAverage(data, frame, disorder):\n",
    "    averages = []\n",
    "    for year in data['Year'].unique():\n",
    "        averages.append(data[data['Year'] == year][disorder].mean())\n",
    "    frame[disorder] = averages\n",
    "    return frame\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearlyAverage(data):\n",
    "    frames = []   \n",
    "    for dataframe in data:\n",
    "        frame = pd.DataFrame()\n",
    "        frame['Year'] = dataframe['Year'].unique()\n",
    "        for feature in dataframe.columns[2:]:\n",
    "            frame = globalAverage(dataframe,frame, feature)\n",
    "        frames.append(frame)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(df, sort_by, color_by, dpi=200):\n",
    "    df.sort_values(by=sort_by, inplace=True)\n",
    "    plt.figure(dpi=dpi) \n",
    "    fig = px.bar(df, x=sort_by, y=\"Entity\", orientation='h', color=color_by)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistogram(df, column, title, xlabel, ylabel, figsize=(10, 6), kde=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.histplot(df[column], kde=kde)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# plot_histogram(df, 'Schizophrenia disorders (share of population) - Sex: Both - Age: Age-standardized', \n",
    "#                'Distribution of Schizophrenia Disorder Prevalence', 'Prevalence (Age-standardized)', 'Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotLosses(trainLosses, valLosses, title='Training and Validation Losses', xlabel='Epoch', ylabel='Loss'):\n",
    "\n",
    "    plt.plot(trainLosses, label='Train Loss')\n",
    "    plt.plot(valLosses, label='Val Loss')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# plot_losses(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot predicted vs actual data\n",
    "def plotPredictionsVSActual(xTest, yTest, outputs, interval=(0, 39)):\n",
    "\n",
    "    i = random.randint(*interval)\n",
    "\n",
    "    # Extend the last point in the test sequence with the predicted and actual outcome\n",
    "    predicted = np.append(xTest[i, :, 0], outputs[i])\n",
    "    actual = np.append(xTest[i, :, 0], yTest[i])\n",
    "\n",
    "    # Time points for plotting\n",
    "    x = np.linspace(0, len(predicted) - 1, len(predicted))\n",
    "\n",
    "    # Plotting the series\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x[:-1], actual[:-1], 'r-', label='Actual (History)')\n",
    "    plt.plot(x[-1:], actual[-1:], 'ro', label='Actual (Latest)', markersize=10)\n",
    "    plt.plot(x[:-1], predicted[:-1], 'b-', label='Predicted (History)')\n",
    "    plt.plot(x[-1:], predicted[-1:], 'bo', label='Predicted (Latest)', markersize=10)\n",
    "\n",
    "    # Identify and highlight overlapping regions in the prediction\n",
    "    overlap = np.logical_and(predicted > 0, actual > 0)\n",
    "    plt.plot(x[overlap], actual[overlap], 'k', label='Overlap')\n",
    "\n",
    "    # Adding plot decorations\n",
    "    plt.title('Depression Prediction and Actual Values for a Random Index')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming xTest, yTest, and outputs are defined properly)\n",
    "# plot_prediction_vs_actual(xTest, yTest, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizePattern(data, feature):\n",
    "    pattern = pd.Series(data[feature],index=data['Year'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(pattern.index, pattern.values, marker='o', linestyle='-')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel(feature)\n",
    "    plt.title(f'{feature} over Years')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "for dataframe in data:\n",
    "    for feature in dataframe.columns[2:]:\n",
    "        visualizePattern(dataframe, feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingAverage(data, window):\n",
    "    return data.rolling(window=window).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentialSmoothing(data,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dalia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
